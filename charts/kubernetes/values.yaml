# yaml-language-server: $schema=./values.schema.json
nameOverride: ""
fullnameOverride: ""
labels: {}

kubeconfig: {}

clusterName: kubernetes

prometheus:
  createConnection: true
  connection: prometheus
  url:  # @schema type:object,null
#   value: ''
#   valueFrom: ''

  # prometheus labels to inject: "\,label1=key1\,label2=key2\,label3=~key3"
  # Note: comma(,) needs to be escaped as /, and prometheus.labels should always start with '/,' to work with existing labels
  labels: ''
  # Leave auth blank or use one of [username+password / bearer / oauth]
  auth:  # @schema type:object,null
#   username:
#     valueFrom:
#       secretKeyRef:
#         name:
#         key:
#   password:
#     valueFrom:
#       secretKeyRef:
#         name:
#         key:
#   bearer:
#     valueFrom:
#       secretKeyRef:
#         name:
#         key:
#   oauth:
#     username:
#       valueFrom:
#         secretKeyRef:
#           name:
#           key:
#     password:
#       valueFrom:
#         secretKeyRef:
#           name:
#           key:
#     scopes: []
#     tokenURL: ''
#     params: {}

metrics:
  enabled: true
  # Allowed: [prometheus|gke]
  type: prometheus  # @schema pattern:^(gke|prometheus)$

  queries:
    prometheus:
      cluster_cpu: '1000 * sum(rate(container_cpu_usage_seconds_total{container!=""{{.Values.prometheus.labels}}}[5m]))'
      cluster_memory: 'sum(container_memory_working_set_bytes{container!=""{{.Values.prometheus.labels | default .Values.prometheusLabels}}})'
      node_cpu: '1000 * sum(rate(container_cpu_usage_seconds_total{container!=""{{.Values.prometheus.labels | default .Values.prometheusLabels}}}[5m])) by (node)'
      node_memory: 'sum(container_memory_working_set_bytes{container!="",pod!=""{{.Values.prometheus.labels | default .Values.prometheusLabels}}} * on(pod, namespace) group_left kube_pod_status_phase{phase="Running"{{.Values.prometheus.labels | default .Values.prometheusLabels}}} > 0) by (node)'
      node_storage: 'max by (instance) (avg_over_time(node_filesystem_avail_bytes{mountpoint="/",fstype!="rootfs"{{.Values.prometheus.labels | default .Values.prometheusLabels}}}[5m]))'
      pod_cpu: '1000 * sum(rate(container_cpu_usage_seconds_total{container!=""{{.Values.prometheus.labels | default .Values.prometheusLabels}}}[5m])) by (pod)'
      pod_memory: 'sum(container_memory_working_set_bytes{container!=""{{.Values.prometheus.labels | default .Values.prometheusLabels}}}) by (pod)'
      namespace_cpu: '1000 * sum(rate(container_cpu_usage_seconds_total{container!=""{{.Values.prometheus.labels | default .Values.prometheusLabels}}}[5m])) by (namespace)'
      namespace_memory: 'sum(container_memory_working_set_bytes{container!="",pod!=""{{.Values.prometheus.labels | default .Values.prometheusLabels}}} * on(pod, namespace) group_left kube_pod_status_phase{phase="Running"{{.Values.prometheus.labels | default .Values.prometheusLabels}}} > 0) by (namespace)'
    gke:
      node_cpu:


topology:
  name: cluster
  schedule: "@every 5m"
  icon: kubernetes

scraper:
  name: kubernetes
  schedule: "@every 15m"
  exclusions:
    name: []
    namespace: []
    kind: []
    labels: {}
  defaultExclusions:
    name: []
    namespace: []
    labels:
      canary-checker.flanksource.com/generated: "true"
      canary-checker.flanksource.com/check-id: "*"
      canary-checker.flanksource.com/check: "*"
    kind:
      - Secret
      - APIService
      - PodMetrics
      - NodeMetrics
      - endpoints.discovery.k8s.io
      - endpointslices.discovery.k8s.io
      - leases.coordination.k8s.io
      - podmetrics.metrics.k8s.io
      - nodemetrics.metrics.k8s.io
      - customresourcedefinition
      - controllerrevisions
      - certificaterequests
      - orders.acme.cert-manager.io

  relationships: []
  defaultRelationships:
    - kind:
        expr: "has(spec.claimRef) ? spec.claimRef.kind : ''"
      name:
        expr: "has(spec.claimRef) ? spec.claimRef.name : ''"
      namespace:
        expr: "has(spec.claimRef) ? spec.claimRef.namespace : ''"
    # FluxCD Helm relationships
    - name:
        label: "helm.toolkit.fluxcd.io/name"
      namespace:
        label: "helm.toolkit.fluxcd.io/namespace"
      kind:
        value: "HelmRelease"
    # FluxCD Kustomize relationships
    - name:
        label: "kustomize.toolkit.fluxcd.io/name"
      namespace:
        label: "kustomize.toolkit.fluxcd.io/namespace"
      kind:
        value: "Kustomization"
    # FluxCD Git relationships
    - name:
        expr: "has(spec.sourceRef) ? spec.sourceRef.name : '' "
      namespace:
        expr: "has(spec.sourceRef) && has(spec.sourceRef.namespace)  ? spec.sourceRef.namespace : metadata.namespace "
      kind:
        value: "GitRepository"

  event:
    exclusions:
      reason:
        - SuccessfulCreate
        - Created
        - DNSConfigForming
        - ArtifactUpToDate
        - GarbageCollectionSucceeded

    severityKeywords:
      error:
        - failed
        - error
      warn:
        - backoff
        - nodeoutofmemory
  transform:
    exclude: []
    mask: []
    changes:
      exclude: []
      mapping: []
    expr: ""
    relationship: []
  defaultTransform:
    exclude: []
    mask: []
    changes:
      exclude:
        - 'details.source.component == "canary-checker" && (change_type == "Failed" || change_type == "Pass")'
        - 'change_type == "diff" && config_type == "Kubernetes::Canary" && summary.startsWith("status.")'
        - 'config_type == "Kubernetes::Node" && summary == "status.images"'
        - 'has(details.source) && details.source.component == "kustomize-controller" && details.reason == "ReconciliationSucceeded"'
        - 'config_type.startsWith("Kubernetes::") && summary == "metadata.annotations.endpoints.kubernetes.io/last-change-trigger-time"'
        - >
          change_type == "diff" && summary == "status.reconciledAt" &&
          config != null &&
          has(config.apiVersion) && config.apiVersion == "argoproj.io/v1alpha1" &&
          has(config.kind) && config.kind == "Application"
      mapping:
        - filter: >
            change.change_type == 'diff' && change.summary == "status.containerStatuses" &&
            patch != null && has(patch.status) && has(patch.status.containerStatuses) &&
            patch.status.containerStatuses.size() > 0 &&
            has(patch.status.containerStatuses[0].restartCount)
          type: PodCrashLooping

    relationship:
      # Link Crossplane objects to the cloud resources
      - filter: config_type.startsWith("Crossplane::") && has(config.status) && has(config.status.atProvider)
        external_id:
          expr: config.status.atProvider.id.lowerAscii()
      # Link a service to a deployment (adjust the label selector accordingly)
      - filter: config_type == "Kubernetes::Service"
        type:
          value: "Kubernetes::Deployment"
        name:
          expr: |
            has(config.spec.selector) && has(config.spec.selector.name) ? config.spec.selector.name : ''
      # Link Pods to PVCs
      - filter: config_type == 'Kubernetes::Pod' && has(config.spec) && has(config.spec.volumes)
        expr: |
          config.spec.volumes.
            filter(item, has(item.persistentVolumeClaim)).
            map(item, {"type": "Kubernetes::PersistentVolumeClaim", "name": item.persistentVolumeClaim.claimName}).
            toJSON()
      # Link Argo Application to the resources
      - filter: config_type == "Kubernetes::Application" && config.apiVersion == "argoproj.io/v1alpha1"
        expr: |
          config.status.resources.map(item, {
            "type": "Kubernetes::" + item.kind,
            "name": item.name,
            "labels": {
              "namespace": item.?namespace.orValue(''),
            },
          }).toJSON()
  retention:
    staleItemAge: "4h"
    defaultChanges:
      - name: ReconciliationSucceeded
        age: 1d
      - name: ArtifactUpToDate
        age: 1d
      - name: GarbageCollectionSucceeded
        age: 1d
      - name: Pulling
        age: 1d
      - name: Pulled
        age: 1d
      - name: Killing
        age: 7d
      - name: Scheduled
        age: 7d
      - name: Started
        age: 7d
    changes: []
playbooks:
  # If this is set to false, no playbooks will be created
  # If true, all individual playbook values will be honored
  enabled: true

  deletePod: true
  cleanupFailedPods: true
  podSnapshot: false
  requestNamespaceAccess: true
  restartDeployment: true
  scaleDeployment: true
  createDeployment: true
  updateResourceImage: true
  updateResourceRequestsLimits: true
  deleteResource: true
  edit_kubernetes_manifests:
    enabled: false
    git_connection: "connection://"
